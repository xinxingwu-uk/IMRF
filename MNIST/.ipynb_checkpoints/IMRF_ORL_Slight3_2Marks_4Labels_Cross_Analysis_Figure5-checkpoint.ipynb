{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------Reproducible----------------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import os\n",
    "\n",
    "seed=0\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "np.random.seed(seed)\n",
    "rn.seed(seed)\n",
    "#session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "session_conf =tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "#tf.set_random_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "#sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "\n",
    "K.set_session(sess)\n",
    "#----------------------------Reproducible----------------------------------------------------------------------------------------\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Flatten, Activation, Dropout, Layer\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import to_categorical,plot_model\n",
    "from keras import optimizers,initializers,constraints,regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import LambdaCallback,ModelCheckpoint\n",
    "from sklearn.ensemble import ExtraTreesClassifier,RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score,ShuffleSplit,train_test_split,StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder,OneHotEncoder\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "import scipy.sparse as sparse\n",
    "import pandas as pd\n",
    "import random\n",
    "import h5py\n",
    "import math\n",
    "import gc\n",
    "from functools import reduce\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "#Import ourslef defined methods\n",
    "import sys\n",
    "sys.path.append(r\"./Defined\")\n",
    "import Functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path='./data/faces/ORL_Faces/'\n",
    "\n",
    "original_edge_length=92\n",
    "original_edge_width=112\n",
    "\n",
    "black_area=4\n",
    "\n",
    "number_samples_for_show=30\n",
    "\n",
    "minority1=30\n",
    "minority2=70\n",
    "minority3=40\n",
    "majority=90\n",
    "sampling_times=100\n",
    "seed_range=np.random.randint(100,size=100)#np.arange(0,100,1)\n",
    "p_n_estimators=2100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = {}\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(dataset_path):\n",
    "    dirnames.sort()\n",
    "    filenames.sort()\n",
    "    for filename in [f for f in filenames if f.endswith(\".pgm\") and not f[0] == '.']:\n",
    "        full_path = os.path.join(dirpath, filename)\n",
    "        filename = full_path.split('/')[-1]\n",
    "        file_identifier=\"%s\" % (full_path.split('/')[-3])\n",
    "        #print(file_identifier)\n",
    "        if file_identifier not in points.keys():\n",
    "            points[file_identifier] = []\n",
    "        #image = io.imread(full_path)\n",
    "        image_=Image.open(full_path).resize((original_edge_length, original_edge_width),Image.ANTIALIAS)\n",
    "        image=np.asarray(image_)\n",
    "        points[file_identifier].append(image)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "Feature = []\n",
    "for key in points.keys():\n",
    "    for image_i in range(len(points[key])):\n",
    "        Feature.append(np.array(points[key][image_i]))\n",
    "\n",
    "Features=np.array(Feature)\n",
    "rn.shuffle(Features)\n",
    "\n",
    "Features_part1=Features[0:(400-minority1-minority2-minority3)]\n",
    "Features_part2=Features[(400-minority1-minority2-minority3):(400-minority2-minority3)]\n",
    "Features_part3=Features[(400-minority2-minority3):(400-minority3)]\n",
    "Features_part4=Features[(400-minority3):]\n",
    "\n",
    "Labels=np.r_[np.array([0]*(400-minority1-minority2-minority3)),np.array([1]*minority1),np.array([2]*minority2),np.array([3]*minority3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Add noise to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "Collect1_noise_data=[]\n",
    "for i in np.arange(400-minority1-minority2-minority3):\n",
    "    Collect1_noise_data_i=np.zeros(Features_part2[1].shape)\n",
    "    Collect1_noise_data.append(Collect1_noise_data_i)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "Collect2_noise_data=[]\n",
    "for i in np.arange(0,minority1):\n",
    "    Features_part2_i=Features_part2[i]\n",
    "    \n",
    "    noise_data=Features_part2_i\n",
    "    \n",
    "    Collect2_noise_data_i=np.zeros(Features_part2[1].shape)\n",
    "    \n",
    "    Up_Down_position=random.randint(-3,3)\n",
    "    Left_Right_position=random.randint(-3,3)\n",
    "    #Up_Down_position=0\n",
    "    #Left_Right_position=0\n",
    "    \n",
    "    for m in np.arange(black_area):\n",
    "        for n in np.arange(black_area):\n",
    "            noise_position_1=19+m+Up_Down_position\n",
    "            noise_position_2=19+n+Left_Right_position\n",
    "            \n",
    "            noise_data[noise_position_1,noise_position_2]=0.1\n",
    "            Collect2_noise_data_i[noise_position_1,noise_position_2]=0.1\n",
    "            \n",
    "            noise_position_1=69+m+Up_Down_position\n",
    "            noise_position_2=69+n+Left_Right_position\n",
    "            \n",
    "            noise_data[noise_position_1,noise_position_2]=0.1\n",
    "            Collect2_noise_data_i[noise_position_1,noise_position_2]=0.1\n",
    "            \n",
    "    Collect2_noise_data.append(Collect2_noise_data_i)\n",
    "\n",
    "for i in np.arange(0,minority2):\n",
    "    Features_part3_i=Features_part3[i]\n",
    "    \n",
    "    noise_data=Features_part3_i\n",
    "    \n",
    "    Collect2_noise_data_i=np.zeros(Features_part3[1].shape)\n",
    "    \n",
    "    Up_Down_position=random.randint(-3,3)\n",
    "    Left_Right_position=random.randint(-3,3)\n",
    "    #Up_Down_position=0\n",
    "    #Left_Right_position=0\n",
    "    \n",
    "    for m in np.arange(black_area):\n",
    "        for n in np.arange(black_area):         \n",
    "            noise_position_1=89+m+Up_Down_position\n",
    "            noise_position_2=29+n+Left_Right_position\n",
    "            \n",
    "            noise_data[noise_position_1,noise_position_2]=0.1\n",
    "            Collect2_noise_data_i[noise_position_1,noise_position_2]=0.1\n",
    "            \n",
    "            noise_position_1=69+m+Up_Down_position\n",
    "            noise_position_2=69+n+Left_Right_position\n",
    "            \n",
    "            noise_data[noise_position_1,noise_position_2]=0.1\n",
    "            Collect2_noise_data_i[noise_position_1,noise_position_2]=0.1\n",
    "            \n",
    "    Collect2_noise_data.append(Collect2_noise_data_i)\n",
    "    \n",
    "    \n",
    "for i in np.arange(0,minority3):\n",
    "    Features_part4_i=Features_part4[i]\n",
    "    \n",
    "    noise_data=Features_part4_i\n",
    "    \n",
    "    Collect2_noise_data_i=np.zeros(Features_part4[1].shape)\n",
    "    \n",
    "    Up_Down_position=random.randint(-3,3)\n",
    "    Left_Right_position=random.randint(-3,3)\n",
    "    #Up_Down_position=0\n",
    "    #Left_Right_position=0\n",
    "    \n",
    "    for m in np.arange(black_area):\n",
    "        for n in np.arange(black_area):\n",
    "            noise_position_1=19+m+Up_Down_position\n",
    "            noise_position_2=19+n+Left_Right_position\n",
    "            \n",
    "            noise_data[noise_position_1,noise_position_2]=0.1\n",
    "            Collect2_noise_data_i[noise_position_1,noise_position_2]=0.1   \n",
    "            \n",
    "            noise_position_1=89+m+Up_Down_position\n",
    "            noise_position_2=29+n+Left_Right_position\n",
    "            \n",
    "            noise_data[noise_position_1,noise_position_2]=0.1\n",
    "            Collect2_noise_data_i[noise_position_1,noise_position_2]=0.1\n",
    "            \n",
    "            noise_position_1=69+m+Up_Down_position\n",
    "            noise_position_2=69+n+Left_Right_position\n",
    "            \n",
    "            noise_data[noise_position_1,noise_position_2]=0.1\n",
    "            Collect2_noise_data_i[noise_position_1,noise_position_2]=0.1\n",
    "            \n",
    "    Collect2_noise_data.append(Collect2_noise_data_i)\n",
    "    \n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "Array_Feature = []\n",
    "for i in np.arange(Features.shape[0]):\n",
    "    Array_Feature.append(Features[i].flatten())\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "Collect_noise_data=np.array(Collect1_noise_data+Collect2_noise_data)\n",
    "print('Shape of Collect_noise_data',Collect_noise_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_indices = np.random.permutation(400)\n",
    "\n",
    "Array_Feature_=np.array(Array_Feature)[rand_indices]\n",
    "Labels_=np.array(Labels)[rand_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 The number of samples for different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(4):\n",
    "    print(i,\":\",np.sum(Labels_==i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Do undersampling and generate multiple sample subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Samples_list=[]\n",
    "Labels_for_Samples_list=[]\n",
    "for time_i in np.arange(sampling_times):\n",
    "    seed_i=time_i\n",
    "    rus=RandomUnderSampler(sampling_strategy={0: majority,1:minority1,2:minority2,3:minority3},random_state=seed_i,replacement=True)\n",
    "    Samples_,Labels_for_Samples_=rus.fit_sample(Array_Feature_,Labels_)\n",
    "    Samples_list.append(Samples_)\n",
    "    Labels_for_Samples_list.append(Labels_for_Samples_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Split training and testing samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_list=[]\n",
    "X_test_list=[]\n",
    "y_train_list=[]\n",
    "y_test_list=[]\n",
    "\n",
    "for time_i in np.arange(sampling_times):\n",
    "    Samples_list_i_=Samples_list[time_i]\n",
    "    Labels_for_Samples_i_=Labels_for_Samples_list[time_i]\n",
    "    rand_indices = np.random.permutation(majority+minority1+minority2+minority3)\n",
    "    Samples_list_i=Samples_list_i_[rand_indices]\n",
    "    Labels_for_Samples_i=Labels_for_Samples_i_[rand_indices]\n",
    "    \n",
    "    X_train_i,X_test_i, y_train_i, y_test_i =train_test_split(Samples_list_i,Labels_for_Samples_i,test_size=0.3, random_state=seed)\n",
    "\n",
    "    print('Shape of X_train_i: ' + str(X_train_i.shape))\n",
    "    print('Shape of y_train_i: ' + str(y_train_i.shape))\n",
    "    print('Shape of X_test_i: ' + str(X_test_i.shape))\n",
    "    print('Shape of y_test_i: ' + str(y_test_i.shape))\n",
    "    \n",
    "    X_train_list.append(X_train_i)\n",
    "    X_test_list.append(X_test_i)\n",
    "    y_train_list.append(y_train_i)\n",
    "    y_test_list.append(y_test_i)\n",
    "    \n",
    "    print(\"\\n======================================================\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Display training and testing samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.show_data_figures(np.array(X_train_list[0])[0:number_samples_for_show,:],original_edge_width,original_edge_length,columns = 10,p_pad=-2,p_w_pad=0.15, p_h_pad=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_list[0][0:number_samples_for_show]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.show_data_figures(np.array(X_train_list[1])[0:number_samples_for_show,:],original_edge_width,original_edge_length,columns = 10,p_pad=-2,p_w_pad=0.15, p_h_pad=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_list[1][0:number_samples_for_show]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.show_data_figures(np.array(X_test_list[0])[0:number_samples_for_show,:],original_edge_width,original_edge_length,columns = 10,p_pad=-2,p_w_pad=0.15, p_h_pad=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_list[0][0:number_samples_for_show]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.show_data_figures(np.array(X_test_list[1])[0:number_samples_for_show,:],original_edge_width,original_edge_length,columns = 10,p_pad=-2,p_w_pad=0.15, p_h_pad=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_list[1][0:number_samples_for_show]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Improt predicated results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_predicated_results=\"./RF_results/predicated_results_sl3_2M_4L_Cross.csv\"\n",
    "predicated_results=pd.read_csv(path_predicated_results, sep='delimiter', header=None)\n",
    "class_names=[\"Clean fact\",\"N1-Clean fact\",\"N2-Clean fact\",\"N12-Clean fact\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Average accuarcy and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_i=int(predicated_results.shape[0]/(4*len(seed_range)))\n",
    "print(\"The number of samplings: \",time_i)\n",
    "\n",
    "Acc_train_ave_collect=[]\n",
    "Acc_test_ave_collect=[]\n",
    "conf_martix_ave_collect=[]\n",
    "report_matrix_ave_collect=[]\n",
    "\n",
    "for time_i in np.arange(time_i):\n",
    "    print(\"Sampling \"+str(time_i+1)+\"----------------------------------------------------------------------------------\\n\")\n",
    "    \n",
    "    Acc_train_list=[]\n",
    "    Acc_test_list=[]\n",
    "\n",
    "    conf_martix=[]\n",
    "    report_matrix=[]\n",
    "\n",
    "    for pred_i in np.arange(len(seed_range)*time_i,len(seed_range)*(time_i+1)):\n",
    "    \n",
    "        Acc_train=np.array(predicated_results)[4*pred_i]\n",
    "        Acc_test=np.array(predicated_results)[4*pred_i+1]\n",
    "        p_y_pred=np.array(predicated_results)[4*pred_i+2]\n",
    "        p_y_test=np.array(predicated_results)[4*pred_i+3]\n",
    "        \n",
    "        Acc_train_list.append(Acc_train)\n",
    "        Acc_test_list.append(Acc_test)\n",
    "    \n",
    "        pred_set=np.array(p_y_pred[0].split(\",\")).astype(float)\n",
    "        truth_set=np.array(p_y_test[0].split(\",\")).astype(float)\n",
    "\n",
    "        conf_martix_i = np.array(confusion_matrix(truth_set, pred_set))\n",
    "        conf_martix.append(conf_martix_i)\n",
    "\n",
    "        report = classification_report(truth_set, pred_set)\n",
    "        report_matrix_i=[]\n",
    "        report_matrix_i.append(np.array(report.split('\\n')[2].split()[1:]).astype(float))\n",
    "        report_matrix_i.append(np.array(report.split('\\n')[3].split()[1:]).astype(float))\n",
    "        report_matrix_i.append(np.array(report.split('\\n')[4].split()[1:]).astype(float))\n",
    "        report_matrix_i.append(np.array(report.split('\\n')[5].split()[1:]).astype(float))\n",
    "        report_matrix_i.append(np.array(['nan','nan']+report.split('\\n')[7].split()[1:]).astype(float))\n",
    "        report_matrix_i.append(np.array(report.split('\\n')[8].split()[2:]).astype(float))\n",
    "        report_matrix_i.append(np.array(report.split('\\n')[9].split()[2:]).astype(float))\n",
    "\n",
    "        report_matrix.append(np.array(report_matrix_i))\n",
    "    \n",
    "    # Accuarcy---------------------------------------------------------------------------------------------------------\n",
    "    Acc_train_ave=np.average(np.array(Acc_train_list).astype(float))\n",
    "    Acc_test_ave=np.average(np.array(Acc_test_list).astype(float))\n",
    "    print(\"Acc_train_ave: \",Acc_train_ave)\n",
    "    print(\"Acc_test_ave: \",Acc_test_ave)\n",
    "    print('\\n')\n",
    "\n",
    "    Acc_train_ave_collect.append(Acc_train_ave)\n",
    "    Acc_test_ave_collect.append(Acc_test_ave)\n",
    "    \n",
    "    # Confusion martix---------------------------------------------------------------------------------------------------------\n",
    "    conf_martix_ave=np.average(conf_martix,axis=0)\n",
    "    print(\"Confusion martix: \",conf_martix_ave)\n",
    "    print('\\n')\n",
    "\n",
    "    conf_martix_ave_collect.append(conf_martix_ave)\n",
    "\n",
    "    # Report---------------------------------------------------------------------------------------------------------\n",
    "    report_matrix_ave=np.average(report_matrix,axis=0)\n",
    "    Title=np.array(['Classes']+report.split('\\n')[0].split()).reshape(1,5)\n",
    "    report_matrix_ave_=np.c_[np.array(class_names+['accuracy','macro avg','weighted avg']),np.round(report_matrix_ave,decimals=4).astype(str)]\n",
    "\n",
    "    report_matrix_ave_mergetitle=np.r_[np.array(Title),report_matrix_ave_]\n",
    "\n",
    "    for i in report_matrix_ave_mergetitle:\n",
    "        print('\\t\\t'.join(i))\n",
    "        \n",
    "    report_matrix_ave_collect.append(report_matrix_ave)\n",
    "    \n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ave_ConfusionMatrix=(np.round(np.average(conf_martix_ave_collect,axis=0))).astype(int)\n",
    "print(\"Confusion Matrix\\n\",final_ave_ConfusionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['0', '1', '2', '3']\n",
    "fig, ax = plot_confusion_matrix(conf_mat=final_ave_ConfusionMatrix,\n",
    "                                colorbar=True,\n",
    "                                #show_absolute=False,\n",
    "                                #show_normed=True,\n",
    "                                class_names=class_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ave_ReportMatrix=np.round (np.average(report_matrix_ave_collect,axis=0),decimals=3)\n",
    "print(\"Report Matrix\\n\",final_ave_ReportMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Improt preprocessed results (importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_preprocessed_results=\"./RF_results/preprocessed_results_sl3_2M_4L_Cross.csv\"\n",
    "preprocessed_results=np.array(pd.read_csv(path_preprocessed_results,header=None))\n",
    "preprocessed_results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Compute the important features for each undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups=4\n",
    "top_select=120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"preprocessed_results.shape[0]\",preprocessed_results.shape,\"\\n\")\n",
    "\n",
    "group_importance=[]\n",
    "selected_feature_indices=[]\n",
    "\n",
    "memebers=int(preprocessed_results.shape[0]/float(groups))\n",
    "for group_i in np.arange(groups):\n",
    "    group_importance_i=np.sum(preprocessed_results[group_i*memebers:(group_i+1)*memebers,:],axis=0)\n",
    "    indices_i=np.argsort(group_importance_i)[::-1][0:top_select]\n",
    "    \n",
    "    group_importance.append(group_importance_i)\n",
    "    selected_feature_indices.append(indices_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group_i in np.arange(groups):\n",
    "    print(\"The number of group\",group_i)\n",
    "    F.draw_with_importance(group_importance[group_i],original_edge_width,original_edge_length,top_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.draw_with_importance(np.mean(group_importance,axis=0),original_edge_width,original_edge_length,top_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Compute the common important features from each undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interect_indices=F.intersect_2Dlist(selected_feature_indices)\n",
    "background=np.zeros(original_edge_length*original_edge_width)\n",
    "background[F.intersect_2Dlist(selected_feature_indices)]=group_importance[0][interect_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_feature_catch=[]\n",
    "for interect_indices_i in interect_indices:\n",
    "    common_feature_catch.append([int(interect_indices_i/(original_edge_length)),interect_indices_i%(original_edge_length)])\n",
    "\n",
    "for group_i in np.arange(groups):\n",
    "    print(\"The number of group\",group_i)\n",
    "    F.show_one_figure_with_keyfeature(group_importance[group_i],common_feature_catch,original_edge_width,original_edge_length)\n",
    "    \n",
    "F.show_one_figure_with_keyfeature(np.average(np.array(group_importance),axis=0),common_feature_catch,original_edge_width,original_edge_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(background.reshape(original_edge_width,original_edge_length))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(background.reshape(original_edge_width,original_edge_length),plt.cm.gray)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Show the common important features from each undersampling and comparing with the original key features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.show_one_figure_with_keyfeature(Collect_noise_data.sum(axis=0),common_feature_catch,original_edge_width,original_edge_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Collect_noise_data.sum(axis=0),plt.cm.gray)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Collect_noise_data.sum(axis=0),plt.cm.gray)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
